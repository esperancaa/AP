{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### web scraping (só pelo o id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Blast import NCBIXML \n",
    "from Bio.Blast import NCBIWWW\n",
    "import requests, sys, json\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "from Bio import Entrez\n",
    "from Bio import Medline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from Bio import SearchIO\n",
    "from Bio.SwissProt import KeyWList\n",
    "from Bio import SwissProt\n",
    "from Bio import ExPASy\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Align import MultipleSeqAlignment\n",
    "from Bio.Seq import Seq\n",
    "from Bio import AlignIO\n",
    "from Bio.Align import AlignInfo\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation\n",
    "from Bio.Phylo.TreeConstruction import DistanceCalculator\n",
    "from Bio.Phylo.TreeConstruction import DistanceTreeConstructor\n",
    "from Bio import Phylo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#não interessa\n",
    "import time\n",
    "def url_get(i):\n",
    "    url_list= [ ]\n",
    "    for id in range(243,(243+i)):\n",
    "        url = \"https://www.ncbi.nlm.nih.gov/nuccore/NG_009{}\".format( id )\n",
    "        url_list.append(url)\n",
    "    return url_list\n",
    "url_get(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "content = []\n",
    "for url in url_get(4):\n",
    "    r = requests.get(url)\n",
    "    content.append(r.content)\n",
    "#print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parsing the HTML\n",
    "for c in content:\n",
    "    soup = BeautifulSoup(c, 'html.parser')\n",
    "\n",
    "    # Procurar um tag meta com um determinado atributo\n",
    "    lines = soup.find_all('meta', {'name':\"ncbi_uidlist\"} )\n",
    "\n",
    "    id = \"\"\n",
    "    url = \"\"\n",
    "    for line in lines:\n",
    "#         print(line)\n",
    "#         if 'name' in line.attrs:\n",
    "#             print(line.attrs['name'])\n",
    "        if 'content' in line.attrs:\n",
    "#             print(line.attrs['content'])\t\t\n",
    "            id = line.attrs['content']\n",
    "    if id:\n",
    "        url = \"https://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?id={}&db=nuccore&report=genbank&conwithfeat=on&hide-cdd=on&retmode=text&maxdownloadsize=5000000\".format(id)\n",
    "\n",
    "    r2 = requests.get( url )\n",
    "    r3= r2.content.decode('utf-8')\n",
    "    r3= str(r2.content)\n",
    "    print (r3, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "#existe = re.findall(r\"ORGANISM\\s+.*?(?=bp)\", r3, re.DOTALL)\n",
    "#if existe:\n",
    "    #for definition in existe:\n",
    "        #m = re.match( r\"ORGANISM\\s+(.+)\", definition, re.DOTALL )\n",
    "        #print( re.sub(r'\\s+', ' ', m.group(1) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### web scraping (por condição)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "query= input('escolha o que quer pesquisar: ')\n",
    "def url_get(i):\n",
    "    url_list= [ ]\n",
    "    url = \"https://www.ncbi.nlm.nih.gov/gene/?term={}\".format( query )\n",
    "    url_list.append(url)\n",
    "    return url_list\n",
    "url_get(1)\n",
    "\n",
    "import requests, sys, json\n",
    "content = []\n",
    "for url in url_get(1):\n",
    "    r = requests.get(url)\n",
    "    content.append(r.content)\n",
    "#print(content)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parsing the HTML\n",
    "for c in content:\n",
    "    soup = BeautifulSoup(c, 'html.parser')\n",
    "    a= soup.get_text()\n",
    "    #print (a)\n",
    "\n",
    "import re\n",
    "existe = re.findall(r\"ID:\\s+\\d*(?=\\D)\", a, re.DOTALL)\n",
    "\n",
    "c= ', '.join(existe)\n",
    "h= c.replace('ID: ','')\n",
    "IDS= h.split(', ')\n",
    "\n",
    "n_genes= IDS[0:int(input('escolha o nº de genes que quer obter (máximo 20): '))]\n",
    "#print(n_genes)\n",
    "\n",
    "if n_genes == ['']:\n",
    "    print (\"Não encontramos resultado para a sua pesquisa\")\n",
    "else:\n",
    "    print (n_genes)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar Links dos genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def url_get_id(i):\n",
    "    url_list= [ ]\n",
    "    for id in n_genes:\n",
    "        url = \"https://www.ncbi.nlm.nih.gov/nuccore/{}\".format( id )\n",
    "        url_list.append(url)\n",
    "    return url_list\n",
    "url_get_id(n_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "content_id = []\n",
    "for url in url_get_id(n_genes):\n",
    "    r = requests.get(url)\n",
    "    content_id.append(r.content)\n",
    "print(content_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parsing the HTML\n",
    "for c in content_id:\n",
    "    soup_id = BeautifulSoup(c, 'html.parser')\n",
    "\n",
    "    # Procurar um tag meta com um determinado atributo\n",
    "    lines_id = soup_id.find_all('meta', {'name':\"ncbi_uidlist\"} )\n",
    "\n",
    "    id_id = \"\"\n",
    "    url_id = \"\"\n",
    "    for line in lines_id:\n",
    "        print(line)\n",
    "        if 'name' in line.attrs:\n",
    "            print(line.attrs['name'])\n",
    "        if 'content' in line.attrs:\n",
    "            print(line.attrs['content'])\t\t\n",
    "            id_id = line.attrs['content']\n",
    "    if id:\n",
    "        url_id = \"https://www.ncbi.nlm.nih.gov/sviewer/viewer.fcgi?id={}&db=nuccore&report=genbank&conwithfeat=on&hide-cdd=on&retmode=text&maxdownloadsize=5000000\".format(id_id)\n",
    "\n",
    "    r2 = requests.get(url_id)\n",
    "    r4= r2.content.decode('utf-8')\n",
    "    print (r4, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Ids=[]\n",
    "database = 'nucleotide'\n",
    "email= 'rodrigoce9@gmail.com'\n",
    "idlist= n_genes\n",
    "handle = Entrez.efetch(db=database, id=idlist, rettype=\"gb\") \n",
    "records = list(SeqIO.parse(handle,\"gb\"))\n",
    "handle.close()\n",
    "for info in records:\n",
    "    Ids.append(info.id)\n",
    "    print(info.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar quantidade de cds e a sua localização:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ids=[]\n",
    "database = 'nucleotide'\n",
    "email= 'rodrigoce9@gmail.com'\n",
    "idlist= n_genes\n",
    "handle = Entrez.efetch(db=database, id=idlist, rettype=\"gb\") \n",
    "records = list(SeqIO.parse(handle,\"gb\"))\n",
    "handle.close()\n",
    "featcds = [ ]\n",
    "position=0\n",
    "pos=[]\n",
    "for info in records:\n",
    "    #print(info.id)\n",
    "    for i in range(len(info.features)):\n",
    "        if info.features[i].type == \"CDS\":\n",
    "            featcds.append(i)\n",
    "            cds= i\n",
    "            a= str(cds)\n",
    "            print (len(a))\n",
    "            position=i\n",
    "            pos.append(info.features[position].location)\n",
    "            print (pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar organismos: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Organismos=[]\n",
    "database = 'nucleotide'\n",
    "email= 'rodrigoce9@gmail.com'\n",
    "idlist= n_genes\n",
    "handle = Entrez.efetch(db=database, id=idlist, rettype=\"gb\") \n",
    "records = list(SeqIO.parse(handle,\"gb\"))\n",
    "handle.close()\n",
    "for info in records:\n",
    "    Ids.append(info.id)\n",
    "    print(info.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar Ids de proteinas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pro=[]\n",
    "database = 'nucleotide'\n",
    "email= 'rodrigoce9@gmail.com'\n",
    "idlist= n_genes\n",
    "handle = Entrez.efetch(db=database, id=idlist, rettype=\"gb\") \n",
    "records = list(SeqIO.parse(handle,\"gb\"))\n",
    "handle.close()\n",
    "for info in records:\n",
    "    for i in info.features:\n",
    "        if i.type == \"CDS\":\n",
    "            \n",
    "            pro= str(i.qualifiers[\"protein_id\"])\n",
    "            list_pro.append(pro)\n",
    "div= ', '.join(list_pro)\n",
    "h= div.replace(\"['\",'')\n",
    "hh= h.replace(\"']\",'')\n",
    "ID_PROT= hh.split(', ')\n",
    "print (ID_PROT) # is used below in Uniprot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar Links de proteinas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "ID_PROT\n",
    "def url_get_id_p(a):\n",
    "    url_list_p=[ ]\n",
    "    for id_p in ID_PROT:\n",
    "        url_id_p= \"https://www.uniprot.org/uniprotkb?query={}\".format(id_p)\n",
    "        url_list_p.append(url_id_p)\n",
    "    return url_list_p\n",
    "url_ids_protein=url_get_id_p(ID_PROT)\n",
    "print(url_ids_protein)  # but we don't need to save this links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #if the output is empty, it means that the protein is not reviewed \n",
    "# fields=\"accession\"\n",
    "# WEBSITE_API=\"https://rest.uniprot.org\"\n",
    "# accession=[]\n",
    "\n",
    "# for i in ID_PROT:\n",
    "#     r=get_url(\"{}/uniprotkb/search?query={} AND (reviewed:true)&fields={}&size=1&format=tsv\".format(WEBSITE_API,i,fields))\n",
    "#     accession.append(str(r.content))\n",
    "   \n",
    "# for x in accession:\n",
    "#     print(x)\n",
    "\n",
    "    \n",
    "# sub_location=[]\n",
    "\n",
    "# for i in ID_PROT:\n",
    "#     r=get_url(\"{}/uniprotkb/search?query={} AND (reviewed:true)&fields={}&size=1&format=tsv\".format(WEBSITE_API,i,fields))\n",
    "#     sub_location.append(str(r.content))\n",
    "   \n",
    "# for x in sub_location:\n",
    "#     print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url(url, **kwargs):\n",
    "    response = requests.get(url, **kwargs);\n",
    "\n",
    "    if not response.ok:\n",
    "        print(response.text)\n",
    "        response.raise_for_status()\n",
    "        sys.exit()\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final version\n",
    "WEBSITE_API = \"https://rest.uniprot.org\"\n",
    "fields = [\"accession\",\"organism_name\",\"protein_name\",\"cc_subcellular_location\",\"cc_function\", \"sequence\"]\n",
    "\n",
    "def get_field_for_id(ID_PROT, field):\n",
    "    response = get_url(\"{}/uniprotkb/search?query={} AND (reviewed:true)&fields={}&size=1&format=tsv\".format(WEBSITE_API,ID_PROT,field))\n",
    "    return str(response.content)\n",
    "\n",
    "\n",
    "result = {}\n",
    "for field in fields:\n",
    "    result[field] = [get_field_for_id(i, field) for i in ID_PROT]\n",
    "\n",
    "for field, values in result.items():\n",
    "#     print(f\"Results for {field}\")\n",
    "#     print(values)\n",
    "    #regex\n",
    "    for x in values:\n",
    "        entry = re.search(r'b\\'Entry\\\\n(.+?(?=\\\\n\\'))', x, re.DOTALL)\n",
    "        organism = re.search(r'b\\'Organism\\\\n(.+?(?=\\\\n\\'))', x, re.DOTALL)\n",
    "        function = re.search( r'b\\'Function \\[(CC)\\]\\\\n.{9} (.+?(?=\\\\n\\'))', x, re.DOTALL )\n",
    "        location = re.search( r'b\\'Subcellular location .{27} (.+?(?=\\\\n\\'))', x, re.DOTALL )\n",
    "        sequence = re.search(r'b\\'Sequence\\\\n(.+?(?=\\\\n\\'))', x, re.DOTALL)\n",
    "        if entry:\n",
    "            e = re.match(r'b\\'Entry\\\\n(.+?(?=\\\\n\\'))', x, re.DOTALL )\n",
    "            print(e.group(1))\n",
    "        elif organism:\n",
    "            o = re.match(r'b\\'Organism\\\\n(.+?(?=\\\\n\\'))', x, re.DOTALL )\n",
    "            print(o.group(1))\n",
    "        elif function:\n",
    "            f = re.match(r'b\\'Function .{15} (.+?(?=\\\\n\\'))', x, re.DOTALL )\n",
    "            print(f.group(1))\n",
    "        elif location:\n",
    "            l = re.match(r'b\\'Subcellular location .{27} (.+?(?=\\\\n\\'))', x, re.DOTALL )\n",
    "            print(l.group(1))\n",
    "        elif sequence:\n",
    "            s = re.match(r'b\\'Sequence\\\\n(.+?(?=\\\\n\\'))', x, re.DOTALL)\n",
    "            print(s.group(1) , len(s.group(1)) )\n",
    "#         else:\n",
    "#             print(\"fail\")\n",
    "#ADD CONDITION FOR EMPY RESULT (MEANS THE ID PROTEIN IS NOT REVIEWED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for a in ID_PROT:\n",
    "#     with ExPASy.get_sprot_raw(a) as handle:\n",
    "#             seq_record = SeqIO.read(handle, \"nt\")\n",
    "#             print(seq_record.id)\n",
    "#             print(seq_record.seq)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "content_id_p = []\n",
    "for url in url_get_id_p(len(ID_PROT)):\n",
    "    rp = requests.get(url)\n",
    "    content_id_p.append(rp.content)\n",
    "#print(content_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Parsing the HTML\n",
    "for c in content_id_p:\n",
    "    soup_id_p = BeautifulSoup(c, 'html.parser')\n",
    "\n",
    "    # Procurar um tag meta com um determinado atributo\n",
    "    lines_id_p = soup_id_p.find_all('meta', {'name':\"id\"} )\n",
    "\n",
    "    id_id_p= \"\"\n",
    "    url_id_p = \"\"\n",
    "    for line in lines_id_p:\n",
    "        print(line)\n",
    "        if 'name' in line.attrs:\n",
    "            print(line.attrs['name'])\n",
    "        if 'content' in line.attrs:\n",
    "            print(line.attrs['content'])\t\t\n",
    "            id_id_p = line.attrs['content']\n",
    "    if id_id_p:\n",
    "        url_id_p = \"https://rest.uniprot.org/uniprotkb/search?query={} AND (reviewed:true)&fields='id'&size=1&format=tsv\".format(id_id_p)\n",
    "\n",
    "    r7 = requests.get(url_id_p)\n",
    "    r8= r7.content.decode('utf-8')\n",
    "    print (r8, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar seq:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Biopython\n",
    "Ids=[]\n",
    "database = 'nucleotide'\n",
    "email= 'rodrigoce9@gmail.com'\n",
    "idlist= n_genes\n",
    "handle = Entrez.efetch(db=database, id=idlist, rettype=\"gb\") \n",
    "records = list(SeqIO.parse(handle,\"gb\"))\n",
    "handle.close()\n",
    "seq=[]\n",
    "for info in records:\n",
    "    print(info.id)\n",
    "    print(info.seq[0:50],'...',info.seq[-10:])\n",
    "    print()\n",
    "    seq.append(f'{info.seq[0:50]}...{info.seq[-10:]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar gene description:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ids=[]\n",
    "database = 'nucleotide'\n",
    "email= 'rodrigoce9@gmail.com'\n",
    "idlist= n_genes\n",
    "handle = Entrez.efetch(db=database, id=idlist, rettype=\"gb\") \n",
    "records = list(SeqIO.parse(handle,\"gb\"))\n",
    "handle.close()\n",
    "description=[]\n",
    "for info in records:\n",
    "    print(info.id)\n",
    "    print(info.description)\n",
    "    description.append(info.description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buscar artigos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ids=[]\n",
    "database = 'PubMed'\n",
    "email= 'rodrigoce9@gmail.com'\n",
    "idlist= n_genes\n",
    "handle = Entrez.efetch(db=database, id=idlist, rettype=\"medline\", retmode=\"text\") \n",
    "records = Medline.parse(handle)\n",
    "description=[]\n",
    "for info in records:\n",
    "    print(\"title:\", info.get(\"TI\", \"-\"))\n",
    "    print('authors: ', info.get(\"AU\", \"-\"))\n",
    "    print(\"source:\", info.get(\"SO\", \"-\"))\n",
    "    print(\"affiliation: \", info.get(\"AD\", \"-\") ) # FAZER O IF/ELSE PARA O CASO DE NÃO TER AFFILIATION\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Jorge Gustavo Rocha"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
